Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
30000,2.6651297,0.08372231,0.0009304519,0.068786435,0.0002995469,0.19984895,0.009984911,999.0,0.18500000469308966,0.18500000469308966,1.0
60000,2.418727,0.08734718,0.00019233781,0.06816642,0.0002986913,0.19956377,0.009956421,999.0,0.41022857539355756,0.41022857539355756,1.0
90000,2.0862677,0.049920127,0.0001348548,0.06871356,0.0002977922,0.19926408,0.009926479,999.0,0.46344000320881606,0.46344000320881606,1.0
120000,1.9056978,0.056820206,7.892774e-05,0.06967546,0.00029689135,0.19896379,0.00989648,999.0,0.5329428633408887,0.5329428633408887,1.0
150000,1.3810492,0.101774395,0.00012767846,0.07052465,0.00029599218,0.19866405,0.009866541,999.0,0.7018800163269043,0.7018800163269043,1.0
180000,1.047985,0.08729802,9.923855e-05,0.07204603,0.00029509133,0.19836377,0.009836541,999.0,0.8015714616614527,0.8015714616614527,1.0
