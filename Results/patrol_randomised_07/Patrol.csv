Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Policy/Curiosity Value Estimate,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Losses/Curiosity Forward Loss,Losses/Curiosity Inverse Loss,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Policy/Curiosity Reward,Is Training
30000,0.8056347,0.058618575,0.23580654,9.3929426e-05,0.06831373,0.0002995469,0.19984895,0.009984911,0.025421897,0.024125507,999.0,0.6091666772651175,0.6091666772651175,2.119479260717829,1.0
60000,0.7220133,0.055761836,0.2555064,5.086966e-05,0.06765638,0.0002986913,0.19956377,0.009956421,0.018813975,0.018095423,999.0,0.6016285855289815,0.6016285855289815,2.3306268943207606,1.0
90000,0.7319753,0.059720818,0.24881297,7.092451e-05,0.069291785,0.0002977922,0.19926408,0.009926479,0.019756462,0.017417453,999.0,0.5883600036241114,0.5883600036241114,2.7695738047361376,1.0
120000,0.8409903,0.052250937,0.28821272,8.3882995e-05,0.06957026,0.00029689135,0.19896379,0.00989648,0.024093412,0.024788847,999.0,0.575542863611398,0.575542863611398,2.8711134497608457,1.0
150000,0.97050977,0.05490343,0.26563975,6.0520942e-05,0.07113514,0.00029599218,0.19866405,0.009866541,0.020195264,0.014027466,999.0,0.5136800087615847,0.5136800087615847,2.4339393576979638,1.0
180000,0.92060024,0.054247204,0.25823712,9.670471e-05,0.069666535,0.00029509133,0.19836377,0.009836541,0.022115715,0.018597929,999.0,0.6078571636761938,0.6078571636761938,2.8380713854517254,1.0
210000,1.1168531,0.05058453,0.33517143,0.000101225414,0.07182884,0.0002941922,0.19806407,0.009806599,0.026118746,0.033714984,999.0,0.5153200178593397,0.5153200178593397,3.3684920501708984,1.0
240000,0.96051717,0.057168942,0.31743956,6.313367e-05,0.07106694,0.00029329132,0.19776377,0.0097766,0.021155719,0.020834388,999.0,0.5967142983738866,0.5967142983738866,2.614920892885753,1.0
